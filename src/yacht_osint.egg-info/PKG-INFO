Metadata-Version: 2.4
Name: yacht_osint
Version: 0.1.0
Summary: Automated OSINT scraper for super-yachts
Author: Tenderworks Prodigy
Requires-Python: >=3.11
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: requests==2.31.0
Requires-Dist: cloudscraper==1.2.71
Requires-Dist: feedparser==6.0.11
Requires-Dist: beautifulsoup4==4.12.3
Requires-Dist: lxml==4.9.3
Requires-Dist: trafilatura==1.6.1
Requires-Dist: waybackpy==3.0.6
Requires-Dist: pandas==2.1.4
Requires-Dist: duckdb==1.1.2
Requires-Dist: pyarrow==14.0.2
Requires-Dist: numpy==1.26.4
Requires-Dist: scikit-learn==1.4.2
Requires-Dist: pydantic==1.10.12
Requires-Dist: jsonschema==4.19.2
Requires-Dist: jinja2==3.1.2
Requires-Dist: tqdm==4.66.2
Requires-Dist: gspread==6.1.2
Requires-Dist: oauth2client==4.1.3
Requires-Dist: pydrive2==1.15.1
Requires-Dist: pandera==0.17.2
Requires-Dist: tenacity==8.2.3
Requires-Dist: python-dotenv==1.0.0
Requires-Dist: feedfinder2==0.0.4
Requires-Dist: tldextract==3.4.0
Requires-Dist: python-json-logger==2.0.7
Provides-Extra: dev
Requires-Dist: pytest==7.4.3; extra == "dev"
Requires-Dist: ruff==0.12.5; extra == "dev"
Requires-Dist: black; extra == "dev"
Requires-Dist: pytest-cov; extra == "dev"
Requires-Dist: vcrpy==5.1.0; extra == "dev"
Dynamic: license-file

# yacht_osint

Automated OSINT scraper for super‑yachts (build ≥2010) and their tenders. The
pipeline collects public articles, extracts structured information and pushes
the resulting CSVs to Google Drive. A shared Google Sheet uses `IMPORTDATA()` to
load these CSVs for analysis.

## Installation

```bash
pip install -r requirements.txt
```

Create a `.env` file in the project root with the required secrets:

```
HF_TOKEN=...
GROQ_API_KEY=...
GOOGLE_CSE_API_KEY=...
GOOGLE_CSE_CX=...
DRIVE_FOLDER_ID=...
SPREADSHEET_ID=...
```

Place your Google service account JSON somewhere locally and set
`GOOGLE_APPLICATION_CREDENTIALS` to its path. Rclone must also be configured
with a remote named `remote`.

## Required secrets

Environment variables and GitHub secrets used by the pipeline:

- `HF_TOKEN`
- `GROQ_API_KEY`
- `RCLONE_CONFIG`
- `GOOGLE_CSE_API_KEY`
- `GOOGLE_CSE_CX`
- `DRIVE_FOLDER_ID`
- `SPREADSHEET_ID`

## Google Sheets formulas

```
=IMPORTDATA("https://raw.githubusercontent.com/tenderworks-prodigy/yacht_osint/main/data/exports/yachts.csv")
=IMPORTDATA("https://raw.githubusercontent.com/tenderworks-prodigy/yacht_osint/main/data/exports/tenders.csv")
=IMPORTDATA("https://raw.githubusercontent.com/tenderworks-prodigy/yacht_osint/main/data/exports/yacht_aliases.csv")
=IMPORTDATA("https://raw.githubusercontent.com/tenderworks-prodigy/yacht_osint/main/data/exports/yacht_events.csv")
=IMPORTDATA("https://raw.githubusercontent.com/tenderworks-prodigy/yacht_osint/main/data/exports/sources.csv")
```

## Pipeline overview

```
RSS → Sitemap → CSE → Crawl → Wayback → Parse → Extract → Dedupe →
DuckDB → Exports → rclone → QA → Report
```

### Automatic source discovery

Search terms for Google Custom Search are defined in `configs/settings.yml` under
`search.queries`. Running `python -m src.cli` will issue these queries, collect
unique domains and attempt RSS feed discovery for each domain. Discovered feeds
are saved to `yacht_osint/data/cache/discovered_feeds.json`.
